<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Ashwin Nanjappa">
  <title>Code Yarns â€“ Error using TensorRT engine for inference</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="../styles.css">
  <!-- Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143311697-1">
  </script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'UA-143311697-1');
  </script>
  <!-- Google Site Search -->
  <script async src="https://cse.google.com/cse.js?cx=003521915278168649297:cryrlg7hahe">
  </script>
</head>
<body>
<table style="width: 100%;">
    <tr>
        <td>
            <a href="..">Code Yarns</a> / <a href="index.html">Tech Blog</a>
        </td>
        <td>
            <div class="gcse-search"></div>
        </td>
    </tr>
</table>
<hr style="margin-top: 0px;" />
<header>
<h1 class="title">Error using TensorRT engine for inference</h1>
<p class="date">2017-08-22 15:09:50+00:00</p>
</header>
<h2 id="problem">Problem</h2>
<p>I used <strong>TensorRT</strong> to convert a <strong>Caffe</strong> model into an engine (plan) file on one computer. When I tried to load this engine (plan) file on another computer and use it for inference using TensorRT, I got this error:</p>
<pre><code>customWinogradConvActLayer.cpp:195: virtual void nvinfer1::cudnn::WinogradConvActLayer::allocateResources(const nvinfer1::cudnn::CommonContext&amp;): Assertion &#39;configIsValid(context)&#39; failed.</code></pre>
<h2 id="solution">Solution</h2>
<p>It turns out that the first computer had a NVIDIA 1080 Ti GPU and the engine had been created for it. The second computer had a NVIDIA K80 GPU. Though, TensorRT documentation is vague about this, it seems like an engine created on a specific GPU can only be used for inference on the same model of GPU!</p>
<p>When I created a plan file on the K80 computer, inference worked fine.</p>
<p><strong>Tried with:</strong> TensorRT 2.1, cuDNN 6.0 and CUDA 8.0</p>
<hr />
<p><a href="rss.xml">RSS</a> - <a href="mailto:codeyarns@gmail.com">Contact</a></p>
</body>
</html>
